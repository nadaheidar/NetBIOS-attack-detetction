# -*- coding: utf-8 -*-
"""NetBIOS_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15NBMgF8myNY6tdeee6efP5cTZO8_N-o4
"""

# from google.colab import drive
# drive.mount('/content/drive/')

#imports
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.metrics import f1_score, accuracy_score, classification_report

path="../data/NetBIOS.csv"

df= pd.read_csv(path)

df.head()

len(df)

#COUNT NOT nan VALUES
df.count()

#count nan values
df.isna().sum()

df_with_no_nulls = df.dropna()

len(df_with_no_nulls)

df_with_no_nulls.apply(pd.Series.nunique)

df_with_no_nulls.head(5)

dropped_data=df_with_no_nulls

dropped_data.drop(dropped_data.columns[[0, 1, 2, 4, 7, 21, 22,85]], axis=1, inplace=True)
del dropped_data[' Fwd Header Length']
# dropped_data



data = dropped_data.sample(frac =.2)
data.head()

len(data)


x = data.iloc[:,:-1]
y = data.iloc[:,-1]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

from imblearn.over_sampling import SMOTE
sm= SMOTE(random_state=2)
x_train_res, y_train_res= sm.fit_resample(x_train, y_train)



#model DecisionTreeClassifier
clf_tree = tree.DecisionTreeClassifier()
clf_tree = clf_tree.fit(x_train_res , y_train_res)
clf_y_pred= clf_tree.predict(x_test)

accuracy_tree = accuracy_score(y_true=y_test, y_pred=clf_y_pred)*100
print("accuracy : ",accuracy_tree,"%")

print(classification_report(y_true=y_test, y_pred=clf_y_pred))




import matplotlib.pyplot as plt
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(clf_tree, x_test, y_test)  
plt.show()



#!pip install catboost

#model CatBoostClassifier
from catboost import CatBoostClassifier
clf_Cat = CatBoostClassifier()
clf_Cat.fit(x_train_res, y_train_res)
#clf_Cat.predict_proba(x_test)
y_pred_cat=clf_Cat.predict(x_test)

accuracy_cat = accuracy_score(y_true=y_test, y_pred=y_pred_cat)*100
print("accuracy : ",accuracy_cat,"%")

print(classification_report(y_true=y_test, y_pred=y_pred_cat))

import matplotlib.pyplot as plt
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(clf_tree, x_test, y_test)
plt.show()





#model RandomForestClassifier

from sklearn.ensemble import RandomForestClassifier
 
clf_rand = RandomForestClassifier(max_depth=50, random_state=0)
clf_rand.fit(x_train_res, y_train_res)
y_pred_rand=clf_rand.predict(x_test)

accuracy_rand = accuracy_score(y_true=y_test, y_pred=y_pred_rand)*100
print("accuracy : ",accuracy_rand,"%")

print(classification_report(y_true=y_test, y_pred=y_pred_rand))





import numpy as np
from sklearn.ensemble import  VotingClassifier


clf1 = tree.DecisionTreeClassifier()
clf2 = CatBoostClassifier()
clf3 = RandomForestClassifier(max_depth=50, random_state=0)

eclf1 = VotingClassifier(estimators=[('DT', clf1), ('CBC', clf2), ('rf', clf3)], voting='soft')
eclf1 = eclf1.fit(x_train_res, y_train_res)
print(eclf1.predict(x_test))

accuracy_rand = accuracy_score(y_true=y_test, y_pred=y_pred_rand)*100
print("accuracy : ",accuracy_rand,"%")

print(classification_report(y_true=y_test, y_pred=y_pred_rand))

import matplotlib.pyplot as plt
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(clf_rand, x_test, y_test)  
plt.show()

import pickle
with open(r"model_vot.sav", 'wb') as file:
        pickle.dump(eclf1, file)



